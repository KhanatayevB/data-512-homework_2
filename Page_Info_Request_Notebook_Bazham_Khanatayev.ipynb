{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bazham Khanatayev \\\n",
    "Data 512 HW_2 \\\n",
    "10.15.2023 \\\n",
    "The purpose of this notebook is to take the articles listed in the us_cities_by_state_SEPT.2023.csv file and get the article page revision ID's. We are using the MediaWiki API to grab this information. The notebook outputs a file called article_data.csv that includes the revision ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import tqdm\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MediaWiki API endpoint URL\n",
    "MEDIAWIKIPEDIA_API_ENDPOINT = \"https://en.wikipedia.org/w/api.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to store the article data and error logs\n",
    "article_data = {}\n",
    "error_log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to test the API before we commit to parsing the entire file. I started with just 10 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to test with just 10 rows and without the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows to process\n",
    "N_ROWS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the US cities by state CSV file\n",
    "with open(\"us_cities_by_state_SEPT.2023.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # Skip header if the CSV has one\n",
    "\n",
    "    progress_bar = tqdm.tqdm(total=N_ROWS)\n",
    "\n",
    "    # Iterate over the rows in the CSV file\n",
    "    # islice is used to work with the iterator since it does not have traditional indices\n",
    "    for i, row in islice(enumerate(reader), N_ROWS):\n",
    "        # Extract just the title from the Wikipedia URL\n",
    "        wikipedia_article_title = row[1]\n",
    "\n",
    "        # Make a page info request to the MediaWiki API\n",
    "        response = requests.get(MEDIAWIKIPEDIA_API_ENDPOINT, params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": wikipedia_article_title,\n",
    "            \"prop\": \"info|revisions\",\n",
    "            \"inprop\": \"url\",\n",
    "            \"rvprop\": \"ids\",\n",
    "            \"rvlimit\": 1  # Only get the latest revision\n",
    "        })\n",
    "\n",
    "        # Parse the JSON response\n",
    "        json_response = response.json()\n",
    "\n",
    "        # Extract the page info data\n",
    "        page_data = next(iter(json_response.get(\"query\", {}).get(\"pages\", {}).values()), {})\n",
    "        full_url = page_data.get(\"fullurl\", \"\")\n",
    "        revision_id = page_data.get(\"revisions\", [{}])[0].get(\"revid\", None) if \"revisions\" in page_data else None\n",
    "        \n",
    "        # log the rows where no revision_id was returned\n",
    "        if revision_id:\n",
    "            article_data[full_url] = {\n",
    "                \"url\": full_url,\n",
    "                \"revision_id\": revision_id\n",
    "            }\n",
    "        else:\n",
    "            error_log.append(full_url)\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Data: {'https://en.wikipedia.org/wiki/Abbeville,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Abbeville,_Alabama', 'revision_id': 1171163550}, 'https://en.wikipedia.org/wiki/Adamsville,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Adamsville,_Alabama', 'revision_id': 1177621427}, 'https://en.wikipedia.org/wiki/Addison,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Addison,_Alabama', 'revision_id': 1168359898}, 'https://en.wikipedia.org/wiki/Akron,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Akron,_Alabama', 'revision_id': 1165909508}, 'https://en.wikipedia.org/wiki/Alabaster,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Alabaster,_Alabama', 'revision_id': 1179139816}, 'https://en.wikipedia.org/wiki/Albertville,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Albertville,_Alabama', 'revision_id': 1179198677}, 'https://en.wikipedia.org/wiki/Alexander_City,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Alexander_City,_Alabama', 'revision_id': 1179140073}, 'https://en.wikipedia.org/wiki/Aliceville,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Aliceville,_Alabama', 'revision_id': 1167792390}, 'https://en.wikipedia.org/wiki/Allgood,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Allgood,_Alabama', 'revision_id': 1165909718}, 'https://en.wikipedia.org/wiki/Altoona,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Altoona,_Alabama', 'revision_id': 1165909823}}\n",
      "\n",
      "Errors: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Article Data:\", article_data)\n",
    "print(\"\\nErrors:\", error_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the title as well and test with 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to store the article data and error logs\n",
    "article_data = {}\n",
    "error_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the US cities by state CSV file\n",
    "with open(\"us_cities_by_state_SEPT.2023.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # Skip header if the CSV has one\n",
    "\n",
    "    progress_bar = tqdm.tqdm(total=N_ROWS)\n",
    "\n",
    "    # Iterate over the rows in the CSV file\n",
    "    # islice is used to work with the iterator since it does not have traditional indices\n",
    "    for i, row in islice(enumerate(reader), N_ROWS):\n",
    "        # Extract just the title from the Wikipedia URL\n",
    "        wikipedia_article_title = row[1]\n",
    "\n",
    "        # Make a page info request to the MediaWiki API\n",
    "        response = requests.get(MEDIAWIKIPEDIA_API_ENDPOINT, params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": wikipedia_article_title,\n",
    "            \"prop\": \"info|revisions\",\n",
    "            \"inprop\": \"url\",\n",
    "            \"rvprop\": \"ids\",\n",
    "            \"rvlimit\": 1  # Only get the latest revision\n",
    "        })\n",
    "\n",
    "        # Parse the JSON response\n",
    "        json_response = response.json()\n",
    "\n",
    "        # Extract the page info data\n",
    "        page_data = next(iter(json_response.get(\"query\", {}).get(\"pages\", {}).values()), {})\n",
    "        full_url = page_data.get(\"fullurl\", \"\")\n",
    "        revision_id = page_data.get(\"revisions\", [{}])[0].get(\"revid\", None) if \"revisions\" in page_data else None\n",
    "        \n",
    "        \n",
    "        # log the rows where no revision_id was returned\n",
    "        if revision_id:\n",
    "            article_data[full_url] = {\n",
    "                \"url\": full_url,\n",
    "                \"revision_id\": revision_id,\n",
    "                \"title\": wikipedia_article_title\n",
    "            }\n",
    "        else:\n",
    "            error_log.append(full_url)\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://en.wikipedia.org/wiki/Abbeville,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Abbeville,_Alabama', 'revision_id': 1171163550, 'title': 'Abbeville, Alabama'}, 'https://en.wikipedia.org/wiki/Adamsville,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Adamsville,_Alabama', 'revision_id': 1177621427, 'title': 'Adamsville, Alabama'}, 'https://en.wikipedia.org/wiki/Addison,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Addison,_Alabama', 'revision_id': 1168359898, 'title': 'Addison, Alabama'}, 'https://en.wikipedia.org/wiki/Akron,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Akron,_Alabama', 'revision_id': 1165909508, 'title': 'Akron, Alabama'}, 'https://en.wikipedia.org/wiki/Alabaster,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Alabaster,_Alabama', 'revision_id': 1179139816, 'title': 'Alabaster, Alabama'}, 'https://en.wikipedia.org/wiki/Albertville,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Albertville,_Alabama', 'revision_id': 1179198677, 'title': 'Albertville, Alabama'}, 'https://en.wikipedia.org/wiki/Alexander_City,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Alexander_City,_Alabama', 'revision_id': 1179140073, 'title': 'Alexander City, Alabama'}, 'https://en.wikipedia.org/wiki/Aliceville,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Aliceville,_Alabama', 'revision_id': 1167792390, 'title': 'Aliceville, Alabama'}, 'https://en.wikipedia.org/wiki/Allgood,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Allgood,_Alabama', 'revision_id': 1165909718, 'title': 'Allgood, Alabama'}, 'https://en.wikipedia.org/wiki/Altoona,_Alabama': {'url': 'https://en.wikipedia.org/wiki/Altoona,_Alabama', 'revision_id': 1165909823, 'title': 'Altoona, Alabama'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the article data\n",
    "print(article_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go through the entire file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to store the article data and error logs\n",
    "article_data = {}\n",
    "error_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows to process (set this to a high number if you want to process all rows)\n",
    "N_ROWS = 22160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the US cities by state CSV file\n",
    "with open(\"us_cities_by_state_SEPT.2023.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # Skip header if the CSV has one\n",
    "\n",
    "    progress_bar = tqdm.tqdm(total=N_ROWS)\n",
    "\n",
    "    # Iterate over the rows in the CSV file\n",
    "    # islice is used to work with the iterator since it does not have traditional indices\n",
    "    for i, row in islice(enumerate(reader), N_ROWS):\n",
    "        # Extract just the title from the Wikipedia URL\n",
    "        wikipedia_article_title = row[1]\n",
    "\n",
    "        # Make a page info request to the MediaWiki API\n",
    "        response = requests.get(MEDIAWIKIPEDIA_API_ENDPOINT, params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": wikipedia_article_title,\n",
    "            \"prop\": \"info|revisions\",\n",
    "            \"inprop\": \"url\",\n",
    "            \"rvprop\": \"ids\",\n",
    "            \"rvlimit\": 1  # Only get the latest revision\n",
    "        })\n",
    "\n",
    "        # Parse the JSON response\n",
    "        json_response = response.json()\n",
    "\n",
    "        # Extract the page info data\n",
    "        page_data = next(iter(json_response.get(\"query\", {}).get(\"pages\", {}).values()), {})\n",
    "        full_url = page_data.get(\"fullurl\", \"\")\n",
    "        revision_id = page_data.get(\"revisions\", [{}])[0].get(\"revid\", None) if \"revisions\" in page_data else None\n",
    "        \n",
    "        # log the rows where no revision_id was returned\n",
    "        if revision_id:\n",
    "            article_data[full_url] = {\n",
    "                \"url\": full_url,\n",
    "                \"revision_id\": revision_id,\n",
    "                \"title\": wikipedia_article_title\n",
    "            }\n",
    "        else:\n",
    "            error_log.append(full_url)\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 22157/22160 [1:22:27<00:00,  4.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open the output CSV file\n",
    "with open(\"article_data.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow([\"Article Title\", \"URL\", \"Revision ID\"])\n",
    "\n",
    "    # Write the article data to the CSV file\n",
    "    for article_url, article_data in article_data.items():\n",
    "        writer.writerow([article_data[\"title\"], article_url, article_data[\"revision_id\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
